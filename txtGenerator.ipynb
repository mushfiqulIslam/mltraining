{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from numpy import array\n",
    "from pickle import dump\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "\t# replace '--' with a space ' '\n",
    "\tdoc = doc.replace('--', ' ')\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split()\n",
    "\t# remove punctuation from each token\n",
    "\ttable = str.maketrans('', '', string.punctuation)\n",
    "\ttokens = [w.translate(table) for w in tokens]\n",
    "\t# remove remaining tokens that are not alphabetic\n",
    "\ttokens = [word for word in tokens if word.isalpha()]\n",
    "\t# make lower case\n",
    "\ttokens = [word.lower() for word in tokens]\n",
    "\treturn tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tokens to file, one dialog per line\n",
    "def save_doc(lines, filename):\n",
    "\tdata = '\\n'.join(lines)\n",
    "\tfile = open(filename, 'w')\n",
    "\tfile.write(data)\n",
    "\tfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Gutenberg’s Alice’s Adventures in Wonderland, by Lewis Carroll\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "almost no restrictions whatsoever.  You may copy it, give it aw\n"
     ]
    }
   ],
   "source": [
    "# load document\n",
    "in_filename = 'wonderland.txt'\n",
    "doc = load_doc(in_filename)\n",
    "print(doc[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['project', 'adventures', 'in', 'wonderland', 'by', 'lewis', 'carroll', 'this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', 'you', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 'reuse', 'it', 'under', 'the', 'terms', 'of', 'the', 'project', 'gutenberg', 'license', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at', 'wwwgutenbergorg', 'title', 'adventures', 'in', 'wonderland', 'author', 'lewis', 'carroll', 'posting', 'date', 'june', 'ebook', 'release', 'date', 'march', 'last', 'updated', 'october', 'language', 'english', 'character', 'set', 'encoding', 'start', 'of', 'this', 'project', 'gutenberg', 'ebook', 'adventures', 'in', 'wonderland', 'adventures', 'in', 'wonderland', 'lewis', 'carroll', 'the', 'millennium', 'fulcrum', 'edition', 'chapter', 'i', 'down', 'the', 'rabbithole', 'alice', 'was', 'beginning', 'to', 'get', 'very', 'tired', 'of', 'sitting', 'by', 'her', 'sister', 'on', 'the', 'bank', 'and', 'of', 'having', 'nothing', 'to', 'do', 'once', 'or', 'twice', 'she', 'had', 'peeped', 'into', 'the', 'book', 'her', 'sister', 'was', 'reading', 'but', 'it', 'had', 'no', 'pictures', 'or', 'conversations', 'in', 'it', 'what', 'is', 'the', 'use', 'of', 'a', 'thought', 'alice', 'pictures', 'or', 'so', 'she', 'was', 'considering', 'in', 'her', 'own', 'mind', 'as', 'well', 'as', 'she', 'could', 'for', 'the', 'hot', 'day', 'made', 'her', 'feel', 'very', 'sleepy', 'and', 'stupid', 'whether', 'the', 'pleasure', 'of', 'making', 'a', 'daisychain', 'would', 'be', 'worth', 'the', 'trouble', 'of', 'getting', 'up', 'and', 'picking', 'the', 'daisies', 'when', 'suddenly', 'a', 'white', 'rabbit', 'with', 'pink', 'eyes']\n",
      "Total Tokens: 26760\n",
      "Unique Tokens: 2904\n"
     ]
    }
   ],
   "source": [
    "# clean document\n",
    "tokens = clean_doc(doc)\n",
    "print(tokens[:200])\n",
    "print('Total Tokens: %d' % len(tokens))\n",
    "print('Unique Tokens: %d' % len(set(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 26709\n"
     ]
    }
   ],
   "source": [
    "# organize into sequences of tokens\n",
    "length = 50 + 1\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "\t# select sequence of tokens\n",
    "\tseq = tokens[i-length:i]\n",
    "\t# convert into a line\n",
    "\tline = ' '.join(seq)\n",
    "\t# store\n",
    "\tsequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tokens to file, one dialog per line\n",
    "def save_doc(lines, filename):\n",
    "\tdata = '\\n'.join(lines)\n",
    "\tfile = open(filename, 'w')\n",
    "\tfile.write(data)\n",
    "\tfile.close()\n",
    "    \n",
    "# save sequences to file\n",
    "out_filename = 'republic_sequences.txt'\n",
    "save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "in_filename = 'republic_sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "# vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate into input and output\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 50)            145250    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2905)              293405    \n",
      "=================================================================\n",
      "Total params: 589,555\n",
      "Trainable params: 589,555\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "26709/26709 [==============================] - 31s 1ms/step - loss: 5.0269 - acc: 0.1234\n",
      "Epoch 2/100\n",
      "26709/26709 [==============================] - 36s 1ms/step - loss: 4.8903 - acc: 0.1252\n",
      "Epoch 3/100\n",
      "26709/26709 [==============================] - 37s 1ms/step - loss: 4.8128 - acc: 0.1291\n",
      "Epoch 4/100\n",
      "26709/26709 [==============================] - 37s 1ms/step - loss: 4.7478 - acc: 0.1297\n",
      "Epoch 5/100\n",
      "26709/26709 [==============================] - 37s 1ms/step - loss: 4.6877 - acc: 0.1353\n",
      "Epoch 6/100\n",
      "26709/26709 [==============================] - 39s 1ms/step - loss: 4.6301 - acc: 0.1396\n",
      "Epoch 7/100\n",
      "26709/26709 [==============================] - 37s 1ms/step - loss: 4.5760 - acc: 0.1413\n",
      "Epoch 8/100\n",
      "26709/26709 [==============================] - 38s 1ms/step - loss: 4.5719 - acc: 0.1415\n",
      "Epoch 9/100\n",
      "26709/26709 [==============================] - 39s 1ms/step - loss: 4.5637 - acc: 0.1424\n",
      "Epoch 10/100\n",
      "26709/26709 [==============================] - 39s 1ms/step - loss: 4.6259 - acc: 0.1374\n",
      "Epoch 11/100\n",
      "26709/26709 [==============================] - 40s 1ms/step - loss: 4.4702 - acc: 0.1475\n",
      "Epoch 12/100\n",
      "26709/26709 [==============================] - 38s 1ms/step - loss: 4.4057 - acc: 0.1501\n",
      "Epoch 13/100\n",
      "26709/26709 [==============================] - 40s 1ms/step - loss: 4.3881 - acc: 0.1533\n",
      "Epoch 14/100\n",
      "26709/26709 [==============================] - 40s 1ms/step - loss: 4.3486 - acc: 0.1546\n",
      "Epoch 15/100\n",
      "26709/26709 [==============================] - 42s 2ms/step - loss: 4.3092 - acc: 0.1553\n",
      "Epoch 16/100\n",
      "26709/26709 [==============================] - 45s 2ms/step - loss: 4.2472 - acc: 0.1586\n",
      "Epoch 17/100\n",
      "26709/26709 [==============================] - 41s 2ms/step - loss: 4.1982 - acc: 0.1622\n",
      "Epoch 18/100\n",
      "26709/26709 [==============================] - 40s 2ms/step - loss: 4.1611 - acc: 0.1646\n",
      "Epoch 19/100\n",
      "26709/26709 [==============================] - 41s 2ms/step - loss: 4.1586 - acc: 0.1672\n",
      "Epoch 20/100\n",
      "26709/26709 [==============================] - 51s 2ms/step - loss: 4.1216 - acc: 0.1669\n",
      "Epoch 21/100\n",
      "26709/26709 [==============================] - 45s 2ms/step - loss: 4.0730 - acc: 0.1726\n",
      "Epoch 22/100\n",
      "26709/26709 [==============================] - 49s 2ms/step - loss: 4.0390 - acc: 0.1723\n",
      "Epoch 23/100\n",
      "26709/26709 [==============================] - 41s 2ms/step - loss: 4.0368 - acc: 0.1730\n",
      "Epoch 24/100\n",
      "26709/26709 [==============================] - 44s 2ms/step - loss: 3.9721 - acc: 0.1789\n",
      "Epoch 25/100\n",
      "26709/26709 [==============================] - 44s 2ms/step - loss: 3.9233 - acc: 0.1833\n",
      "Epoch 26/100\n",
      "26709/26709 [==============================] - 48s 2ms/step - loss: 3.8814 - acc: 0.1890\n",
      "Epoch 27/100\n",
      "26709/26709 [==============================] - 46s 2ms/step - loss: 3.8382 - acc: 0.1944\n",
      "Epoch 28/100\n",
      "26709/26709 [==============================] - 40s 1ms/step - loss: 3.8001 - acc: 0.1959\n",
      "Epoch 29/100\n",
      "26709/26709 [==============================] - 40s 1ms/step - loss: 3.7743 - acc: 0.1990\n",
      "Epoch 30/100\n",
      "26709/26709 [==============================] - 40s 1ms/step - loss: 3.7299 - acc: 0.2038\n",
      "Epoch 31/100\n",
      "26709/26709 [==============================] - 39s 1ms/step - loss: 3.6906 - acc: 0.2099\n",
      "Epoch 32/100\n",
      "26709/26709 [==============================] - 41s 2ms/step - loss: 3.6644 - acc: 0.2137\n",
      "Epoch 33/100\n",
      "26709/26709 [==============================] - 41s 2ms/step - loss: 3.6585 - acc: 0.2154\n",
      "Epoch 34/100\n",
      "26709/26709 [==============================] - 46s 2ms/step - loss: 3.6273 - acc: 0.2182\n",
      "Epoch 35/100\n",
      "26709/26709 [==============================] - 46s 2ms/step - loss: 3.5868 - acc: 0.2236\n",
      "Epoch 36/100\n",
      "26709/26709 [==============================] - 49s 2ms/step - loss: 3.6076 - acc: 0.2218\n",
      "Epoch 37/100\n",
      "26709/26709 [==============================] - 49s 2ms/step - loss: 3.5568 - acc: 0.2283\n",
      "Epoch 38/100\n",
      "26709/26709 [==============================] - 50s 2ms/step - loss: 3.5083 - acc: 0.2314\n",
      "Epoch 39/100\n",
      "26709/26709 [==============================] - 49s 2ms/step - loss: 3.4580 - acc: 0.2383\n",
      "Epoch 40/100\n",
      "26709/26709 [==============================] - 51s 2ms/step - loss: 3.4151 - acc: 0.2447\n",
      "Epoch 41/100\n",
      "26709/26709 [==============================] - 48s 2ms/step - loss: 3.3813 - acc: 0.2490\n",
      "Epoch 42/100\n",
      "26709/26709 [==============================] - 42s 2ms/step - loss: 3.3460 - acc: 0.2543\n",
      "Epoch 43/100\n",
      "26709/26709 [==============================] - 39s 1ms/step - loss: 3.3168 - acc: 0.2565\n",
      "Epoch 44/100\n",
      "26709/26709 [==============================] - 39s 1ms/step - loss: 3.2777 - acc: 0.2643\n",
      "Epoch 45/100\n",
      "26709/26709 [==============================] - 38s 1ms/step - loss: 3.2438 - acc: 0.2688\n",
      "Epoch 46/100\n",
      "26709/26709 [==============================] - 45s 2ms/step - loss: 3.2125 - acc: 0.2716\n",
      "Epoch 47/100\n",
      "26709/26709 [==============================] - 50s 2ms/step - loss: 3.1915 - acc: 0.2788\n",
      "Epoch 48/100\n",
      "26709/26709 [==============================] - 52s 2ms/step - loss: 3.1518 - acc: 0.2825\n",
      "Epoch 49/100\n",
      "26709/26709 [==============================] - 52s 2ms/step - loss: 3.1219 - acc: 0.2892\n",
      "Epoch 50/100\n",
      "26709/26709 [==============================] - 50s 2ms/step - loss: 3.0937 - acc: 0.2908\n",
      "Epoch 51/100\n",
      "26709/26709 [==============================] - 53s 2ms/step - loss: 3.0573 - acc: 0.2989\n",
      "Epoch 52/100\n",
      "26709/26709 [==============================] - 49s 2ms/step - loss: 3.0270 - acc: 0.3025\n",
      "Epoch 53/100\n",
      "26709/26709 [==============================] - 47s 2ms/step - loss: 3.0334 - acc: 0.3018\n",
      "Epoch 54/100\n",
      "26709/26709 [==============================] - 50s 2ms/step - loss: 3.0182 - acc: 0.3072\n",
      "Epoch 55/100\n",
      "26709/26709 [==============================] - 42s 2ms/step - loss: 2.9640 - acc: 0.3173\n",
      "Epoch 56/100\n",
      "26709/26709 [==============================] - 39s 1ms/step - loss: 3.0122 - acc: 0.3127\n",
      "Epoch 57/100\n",
      "26709/26709 [==============================] - 39s 1ms/step - loss: 3.0853 - acc: 0.3009\n",
      "Epoch 58/100\n",
      "26709/26709 [==============================] - 46s 2ms/step - loss: 3.0533 - acc: 0.3094\n",
      "Epoch 59/100\n",
      "26709/26709 [==============================] - 53s 2ms/step - loss: 3.0032 - acc: 0.3169\n",
      "Epoch 60/100\n",
      "26709/26709 [==============================] - 46s 2ms/step - loss: 2.9593 - acc: 0.3240\n",
      "Epoch 61/100\n",
      "26709/26709 [==============================] - 44s 2ms/step - loss: 2.9228 - acc: 0.3300\n",
      "Epoch 62/100\n",
      "26709/26709 [==============================] - 54s 2ms/step - loss: 2.8995 - acc: 0.3350\n",
      "Epoch 63/100\n",
      "26709/26709 [==============================] - 51s 2ms/step - loss: 2.8373 - acc: 0.3408\n",
      "Epoch 64/100\n",
      "26709/26709 [==============================] - 47s 2ms/step - loss: 2.8049 - acc: 0.3468\n",
      "Epoch 65/100\n",
      "26709/26709 [==============================] - 52s 2ms/step - loss: 2.8507 - acc: 0.3454\n",
      "Epoch 66/100\n",
      "26709/26709 [==============================] - 40s 1ms/step - loss: 2.9964 - acc: 0.3245\n",
      "Epoch 67/100\n",
      "26709/26709 [==============================] - 40s 1ms/step - loss: 2.9912 - acc: 0.3262\n",
      "Epoch 68/100\n",
      "26709/26709 [==============================] - 43s 2ms/step - loss: 2.9541 - acc: 0.3312\n",
      "Epoch 69/100\n",
      "26709/26709 [==============================] - 46s 2ms/step - loss: 2.8899 - acc: 0.3393\n",
      "Epoch 70/100\n",
      "26709/26709 [==============================] - 47s 2ms/step - loss: 2.8493 - acc: 0.3446\n",
      "Epoch 71/100\n",
      "26709/26709 [==============================] - 51s 2ms/step - loss: 2.7972 - acc: 0.3522\n",
      "Epoch 72/100\n",
      "26709/26709 [==============================] - 49s 2ms/step - loss: 2.7448 - acc: 0.3629\n",
      "Epoch 73/100\n",
      "26709/26709 [==============================] - 50s 2ms/step - loss: 2.7039 - acc: 0.3690\n",
      "Epoch 74/100\n",
      "26709/26709 [==============================] - 47s 2ms/step - loss: 2.6699 - acc: 0.3767\n",
      "Epoch 75/100\n",
      "26709/26709 [==============================] - 41s 2ms/step - loss: 2.7067 - acc: 0.3718\n",
      "Epoch 76/100\n",
      "26709/26709 [==============================] - 41s 2ms/step - loss: 2.7001 - acc: 0.3740\n",
      "Epoch 77/100\n",
      "26709/26709 [==============================] - 43s 2ms/step - loss: 2.6403 - acc: 0.3869\n",
      "Epoch 78/100\n",
      "26709/26709 [==============================] - 40s 1ms/step - loss: 2.6007 - acc: 0.3946\n",
      "Epoch 79/100\n",
      "26709/26709 [==============================] - 40s 2ms/step - loss: 2.5719 - acc: 0.3987\n",
      "Epoch 80/100\n",
      "26709/26709 [==============================] - 43s 2ms/step - loss: 2.5488 - acc: 0.4035\n",
      "Epoch 81/100\n",
      "26709/26709 [==============================] - 39s 1ms/step - loss: 2.5396 - acc: 0.4056\n",
      "Epoch 82/100\n",
      "26709/26709 [==============================] - 38s 1ms/step - loss: 2.5162 - acc: 0.4114\n",
      "Epoch 83/100\n",
      "26709/26709 [==============================] - 38s 1ms/step - loss: 2.4739 - acc: 0.4207\n",
      "Epoch 84/100\n",
      "26709/26709 [==============================] - 38s 1ms/step - loss: 2.4425 - acc: 0.4225\n",
      "Epoch 85/100\n",
      "26709/26709 [==============================] - 38s 1ms/step - loss: 2.4471 - acc: 0.4223\n",
      "Epoch 86/100\n",
      "26709/26709 [==============================] - 38s 1ms/step - loss: 2.4324 - acc: 0.4257\n",
      "Epoch 87/100\n",
      "26709/26709 [==============================] - 38s 1ms/step - loss: 2.4376 - acc: 0.4275\n",
      "Epoch 88/100\n",
      "26709/26709 [==============================] - 38s 1ms/step - loss: 2.4212 - acc: 0.4309\n",
      "Epoch 89/100\n",
      "26709/26709 [==============================] - 37s 1ms/step - loss: 2.4063 - acc: 0.4323\n",
      "Epoch 90/100\n",
      "26709/26709 [==============================] - 38s 1ms/step - loss: 2.4275 - acc: 0.4340\n",
      "Epoch 91/100\n",
      "26709/26709 [==============================] - 39s 1ms/step - loss: 2.4128 - acc: 0.4365\n",
      "Epoch 92/100\n",
      "26709/26709 [==============================] - 41s 2ms/step - loss: 2.3597 - acc: 0.4431\n",
      "Epoch 93/100\n",
      "26709/26709 [==============================] - 40s 2ms/step - loss: 2.3720 - acc: 0.4432\n",
      "Epoch 94/100\n",
      "26709/26709 [==============================] - 43s 2ms/step - loss: 2.3315 - acc: 0.4514\n",
      "Epoch 95/100\n",
      "26709/26709 [==============================] - 43s 2ms/step - loss: 2.2860 - acc: 0.4590\n",
      "Epoch 96/100\n",
      "26709/26709 [==============================] - 40s 2ms/step - loss: 2.2711 - acc: 0.4644\n",
      "Epoch 97/100\n",
      "26709/26709 [==============================] - 42s 2ms/step - loss: 2.2329 - acc: 0.4728\n",
      "Epoch 98/100\n",
      "26709/26709 [==============================] - 40s 1ms/step - loss: 2.2158 - acc: 0.4768\n",
      "Epoch 99/100\n",
      "26709/26709 [==============================] - 40s 2ms/step - loss: 2.1946 - acc: 0.4812\n",
      "Epoch 100/100\n",
      "26709/26709 [==============================] - 40s 1ms/step - loss: 2.1702 - acc: 0.4829\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3baf8d9b00>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "model.fit(X, y, batch_size=128, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to file\n",
    "model.save('model.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = len(lines[0].split()) - 1\n",
    "\n",
    "# load the model\n",
    "model = load_model('model.h5')\n",
    "# load the tokenizer\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old crab took the opportunity of saying to her daughter my dear let this be a lesson to you never to lose your your tongue said the young crab a little snappishly enough to try the patience of an wish i had our dinah here i know i said alice aloud\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select a seed text\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print(seed_text + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "\tresult = list()\n",
    "\tin_text = seed_text\n",
    "\t# generate a fixed number of words\n",
    "\tfor _ in range(n_words):\n",
    "\t\t# encode the text as integer\n",
    "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "\t\t# truncate sequences to a fixed length\n",
    "\t\tencoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "\t\t# predict probabilities for each word\n",
    "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
    "\t\t# map predicted word index to word\n",
    "\t\tout_word = ''\n",
    "\t\tfor word, index in tokenizer.word_index.items():\n",
    "\t\t\tif index == yhat:\n",
    "\t\t\t\tout_word = word\n",
    "\t\t\t\tbreak\n",
    "\t\t# append to input\n",
    "\t\tin_text += ' ' + out_word\n",
    "\t\tresult.append(out_word)\n",
    "\treturn ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "addressing nobody in particular soon fetch that she did not like she had succeeded in cupboards and then the matter worse but would be quite impossible to say and the gryphon lifted up both as the mock turtle said to the jury and the baby the fireirons and reaching your\n"
     ]
    }
   ],
   "source": [
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
